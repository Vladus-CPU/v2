```python
# noobaiXL_colab_improved.ipynb

# -------------------------------------------
# 1. Install and Import Required Libraries
# -------------------------------------------
# Make sure to have "GPU" runtime enabled in your Colab:
# Runtime -> Change runtime type -> Hardware accelerator -> GPU

!pip install --quiet diffusers transformers accelerate safetensors xformers

import torch
import os
from diffusers import StableDiffusionXLPipeline, EulerDiscreteScheduler, EulerAncestralDiscreteScheduler
from diffusers import DPMSolverMultistepScheduler, DDIMScheduler
from PIL import Image
import requests

# -------------------------------------------
# 2. Environment & Resource Checks
# -------------------------------------------
def check_cuda():
    """
    Checks if CUDA/GPU is available and warns the user if it's not.
    """
    if not torch.cuda.is_available():
        print("WARNING: CUDA is not available. Please switch to a GPU runtime for best performance.")
    else:
        print(f"CUDA is available. GPU name: {torch.cuda.get_device_name(0)}")

check_cuda()

# -------------------------------------------
# 3. Model Configuration
# -------------------------------------------
MODEL_ID = "Laxhar/noobai-XL-1.1"

# Schedulers dictionary for easy switching
def create_scheduler_dict():
    """
    Creates a dictionary of different schedulers 
    using the default pipeline configuration as a base.
    """
    # Temporarily load the pipeline to retrieve config
    temp_pipe = StableDiffusionXLPipeline.from_pretrained(
        MODEL_ID,
        torch_dtype=torch.float16,
        variant="fp16",
        use_safetensors=True
    )
    default_scheduler_config = temp_pipe.scheduler.config

    schedulers = {
        "euler": EulerDiscreteScheduler.from_config(default_scheduler_config),
        "euler_ancestral": EulerAncestralDiscreteScheduler.from_config(default_scheduler_config),
        "dpm": DPMSolverMultistepScheduler.from_config(default_scheduler_config),
        "ddim": DDIMScheduler.from_config(default_scheduler_config)
    }
    del temp_pipe
    return schedulers

SCHEDULERS = create_scheduler_dict()

# -------------------------------------------
# 4. Load the Pipeline
# -------------------------------------------
def load_pipeline(model_id=MODEL_ID, scheduler_choice="euler"):
    """
    Loads the Stable Diffusion XL pipeline from Hugging Face Hub,
    sets the scheduler, and removes token length limits.
    """
    print(f"Loading pipeline for model: {model_id}")
    pipe = StableDiffusionXLPipeline.from_pretrained(
        model_id,
        torch_dtype=torch.float16,
        variant="fp16",
        use_safetensors=True
    ).to("cuda")

    # Remove or increase token limit
    pipe.tokenizer.model_max_length = 1000000
    pipe.tokenizer_2.model_max_length = 1000000

    # Set an initial scheduler
    set_scheduler(pipe, scheduler_choice)
    print("Pipeline loaded successfully!")
    return pipe

def set_scheduler(pipeline, scheduler_name="euler"):
    """
    Assigns a chosen scheduler from the SCHEDULERS dictionary to the pipeline.
    If the scheduler name is unknown, defaults to 'euler'.
    """
    if scheduler_name in SCHEDULERS:
        pipeline.scheduler = SCHEDULERS[scheduler_name]
    else:
        print(f"Unknown scheduler '{scheduler_name}'. Defaulting to 'euler'.")
        pipeline.scheduler = SCHEDULERS["euler"]

# Initialize pipeline
pipe = load_pipeline(MODEL_ID, scheduler_choice="euler")

# -------------------------------------------
# 5. Image Generation Function
# -------------------------------------------
def generate_image(
    prompt,
    negative_prompt=None,
    scheduler_name="euler",
    num_inference_steps=30,
    seed=0,
    guidance_scale=7.5,
    width=512,
    height=512,
    hires_fix=False,
    hires_scale=2,
    upscaler_func=None
):
    """
    Generates an image given a text prompt using the loaded pipeline.

    Arguments:
    ----------
    prompt (str): The main text prompt for image generation.
    negative_prompt (str): Prompt content you want the model to avoid.
    scheduler_name (str): Name of the scheduler to use. Options: "euler", "euler_ancestral", "dpm", "ddim".
    num_inference_steps (int): Number of sampling steps.
    seed (int): Random seed for reproducible results.
    guidance_scale (float): Classifier-Free Guidance scale.
    width, height (int): Dimensions of the generated image in pixels.
    hires_fix (bool): If True, will upscale the image after generation.
    hires_scale (int): Scaling factor to apply if hires_fix is True.
    upscaler_func (callable or None): Optional function to handle more advanced upscaling.
                                       If None, uses simple PIL resize.

    Returns:
    --------
    PIL.Image.Image: The generated (and optionally upscaled) image.
    """
    print(f"\n--- Generating Image ---\nPrompt: {prompt}")
    if negative_prompt:
        print(f"Negative Prompt: {negative_prompt}")

    # Set scheduler
    set_scheduler(pipe, scheduler_name)

    # For reproducibility
    generator = torch.Generator("cuda").manual_seed(seed)

    # Generate the image
    image = pipe(
        prompt=prompt,
        negative_prompt=negative_prompt,
        num_inference_steps=num_inference_steps,
        guidance_scale=guidance_scale,
        generator=generator,
        width=width,
        height=height
    ).images[0]

    # Simple condition for hires fix
    if hires_fix:
        print(f"Applying hires fix with scale factor: {hires_scale}x")
        if upscaler_func is not None:
            # Use a custom upscaler function (e.g., ESRGAN or other)
            image = upscaler_func(image, hires_scale)
        else:
            # Simple PIL-based upscale
            new_width = int(width * hires_scale)
            new_height = int(height * hires_scale)
            image = image.resize((new_width, new_height), Image.LANCZOS)
    print("Image generation complete!")
    return image

# -------------------------------------------
# 6. (Optional) Example of a Custom Upscaler
# -------------------------------------------
def example_upscaler_func(image, scale_factor):
    """
    Placeholder for a more advanced upscaling approach 
    (e.g., ESRGAN, Real-ESRGAN, etc.).
    Currently, just demonstrates a placeholder approach 
    and uses the default PIL resize under the hood.
    """
    # NOTE: If you integrate a real library, call 
    # that here instead of simple resize.
    print("Using custom upscaler (placeholder).")
    new_width = int(image.width * scale_factor)
    new_height = int(image.height * scale_factor)
    return image.resize((new_width, new_height), Image.LANCZOS)

# -------------------------------------------
# 7. Example Usage
# -------------------------------------------
# Uncomment and run these lines to generate a test image.

# test_prompt = "A futuristic city skyline at sunset, ultrarealistic"
# test_negative_prompt = "blurry, low quality"
# result_image = generate_image(
#     prompt=test_prompt,
#     negative_prompt=test_negative_prompt,
#     scheduler_name="euler_ancestral",
#     num_inference_steps=25,
#     seed=42,
#     guidance_scale=8.0,
#     width=512,
#     height=512,
#     hires_fix=True,
#     hires_scale=2,
#     upscaler_func=example_upscaler_func  # Or None for basic PIL upscale
# )
# result_image

print("Setup complete. You can now use the generate_image function to create images.")
